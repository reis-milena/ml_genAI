{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network - RNN\n",
    "\n",
    "Time series data:\n",
    "\n",
    "- When you want to predict future behavior based on past behavior\n",
    "\n",
    "For sequence of data of arbitrary lenght:\n",
    "\n",
    "- Machine translation (language data), image captions, machine-generated music (sequence of music notes)\n",
    "\n",
    "![](https://media.geeksforgeeks.org/wp-content/uploads/20231204130132/RNN-vs-FNN-660.png)\n",
    "\n",
    "\"Memory cell\":\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1400/1*SKGAqkVVzT6co-sZ29ze-g.png)\n",
    "\n",
    "- More recent behavior tend to have more influence on the current behavior of the current neuron\n",
    "\n",
    "Layer of recurrent neurons:\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:947/1*K6s4Li0fTl1pSX4-WPBMMA.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Topologies\n",
    "\n",
    "- Sequence to sequence: ex, predict stock prices based on series of historical data\n",
    "\n",
    "- Sequence to vector: ex, words in a sentence sentiment\n",
    "\n",
    "- Vector to sequence: ex, create captions from an image\n",
    "\n",
    "- Encoder -> Decoder (sequence -> vector -> sequence): ex, machine translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training RNN\n",
    "\n",
    "- **Backpropagation through time**: just like backpropagation on MLP's but applied to each time step\n",
    "\n",
    "- All those time steps add up fast:\n",
    "\n",
    "    - Ends up looking like a really, really deep neural network\n",
    "\n",
    "    - Can limit backpropagation to a limited number of times steps (truncated back propagation through time)\n",
    "\n",
    "- State from earlier time steps get diluted over time: this can be a problem, for example when learning sentence structures\n",
    "\n",
    "- Long Short-Term Memory Cell (LSTM Cell): maintains separate short-term and long-term states\n",
    "\n",
    "- Gated Recurrent Unit (GRU): simplified LSTM Cell that performs about as well\n",
    "\n",
    "![](https://www.researchgate.net/publication/332766508/figure/fig4/AS:936071269453826@1600188190644/LSTM-cell-with-its-internal-structure.png)\n",
    "\n",
    "Train RNN is really hard:\n",
    "\n",
    "- Very sensitive to topologies and choice of hyperparameters\n",
    "\n",
    "- Very resource intensive\n",
    "\n",
    "- A wrong choice can lead to a RNN that doesn't converge at all\n",
    "\n",
    "Tip: look for a set of topologies and hyperparameters from previous models that are similar to your application"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
