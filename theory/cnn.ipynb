{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks - CNN\n",
    "\n",
    "- Usually for image recognition and processing\n",
    "\n",
    "\"A Convolutional Neural Network (CNN) is a type of Deep Learning neural network architecture commonly used in Computer Vision. Computer vision is a field of Artificial Intelligence that enables a computer to understand and interpret the image or visual data. \"\n",
    "\n",
    "\"A convolutional neural network (CNN) is a regularized type of feed-forward neural network that learns features by itself via filter (or kernel) optimization\"\n",
    "\n",
    "- Find things in your data where you do not expect to be -> \"feature-location invariant\"\n",
    "\n",
    "- Unstructured data\n",
    "\n",
    "- Very resource-intensive (CPU, GPU and RAM)\n",
    "\n",
    "- Lots of hyperparameters: kernel sizes, many layers with different numbers of units, amount of pooling... all in addition to the usual stuff\n",
    "\n",
    "- Getting the training data is often the hardest part! As well as storing and accessing it\n",
    "\n",
    "Examples:\n",
    "\n",
    "- Images that you want to find features within\n",
    "\n",
    "- Machine translation\n",
    "\n",
    "- Sentence classification\n",
    "\n",
    "- Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How CNN work\n",
    "\n",
    "![](https://editor.analyticsvidhya.com/uploads/59954intro%20to%20CNN.JPG)\n",
    "\n",
    "- Local receptive fields are groups of neurons that only respond to a part of what your eyes see (subsampling)\n",
    "\n",
    "- They overlap each ohter to cover the entire visual field (**convolutions**)\n",
    "\n",
    "- They feed into higher layers that identify increasingly complex images\n",
    "\n",
    "    - Some receptive fields identify horizontal lines at different angles, etc. (filters)\n",
    "\n",
    "    - These would feed into a layer that identifies shapes\n",
    "\n",
    "    - Which might feed into a layer that identifies objects\n",
    "\n",
    "- For color images, extra layers for red, green and blue\n",
    "\n",
    "**Convolution**: break up this data into little chunks and process these chunks individualy, and then they will ensamble a big picture of what you are seeing high up in the chain.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN with Keras\n",
    "\n",
    "- Source data must be on the dimensions: width X lenght X color channels\n",
    "\n",
    "- Conv2D layer type does the actual convolution on a 2D image\n",
    "\n",
    "- MaxPooling2D layers can be used to reduce a 2D layer down by taking the maximum value in a given block\n",
    "\n",
    "- Flatten layers will convert 2D layer to a 1D layer for passing into a flat hidden layer of neurons\n",
    "\n",
    "Typical usage: **Conv2D -> MaxPooling2D -> Dropout -> Flatten -> Dense -> Dropout -> Softmax**\n",
    "\n",
    "* Conv2D for the convolutional of your data\n",
    "\n",
    "* MaxPooling2D to shrink the amount of data\n",
    "\n",
    "* Dropout prevents overfitting\n",
    "\n",
    "* Flatten to fit into a perceptron/layer\n",
    "\n",
    "* Dense is a perceptron\n",
    "\n",
    "* Second droput to avoid overfitting\n",
    "\n",
    "* Softmax to choose the classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specialized CNN Architectures\n",
    "\n",
    "Defines specific arrangement of layers, padding, and hyperparameters\n",
    "\n",
    "- LeNet-5: good for handwritting recognition\n",
    "\n",
    "- AlexNet: image classification, deeper than LeNet\n",
    "\n",
    "- GoogLeNet: even deeper but iwht better perfomance. Introduces inception modules (groups of convolution layers)\n",
    "\n",
    "- ResNet (Residual Network): Even deeper - mantains performance via skip connections"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
