{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "ML Optimization algorithm to achive the local minimun.\n",
    "\n",
    "\"Gradient descent is an optimization algorithm that's used when training a machine learning model. It's based on a convex function and tweaks its parameters iteratively to minimize a given function to its local minimum.\"\n",
    "\n",
    "How it works: choose random parameter values ​​and check the accuracy, then go to close parameter values ​​and check the accuracy and see if the loss function results have decreased. It keeps repeating iteratively that the unit reaches the minimum.\n",
    "\n",
    "Problem: how make sure that the local minimum is the global minimum?\n",
    "\n",
    "    - In practice is not a big problem\n",
    "\n",
    "## Autodiff (rervese mode autodiff)\n",
    "\n",
    "- Technique to speed Gradient Descent.\n",
    "\n",
    "\"Autodiff is a set of techniques to evaluate the partial derivative of a function specified by a computer program.\"\n",
    "\n",
    "\"Automatic differentiation exploits the fact that every computer calculation, no matter how complicated, executes a sequence of elementary arithmetic operations (addition, subtraction, multiplication, division, etc.) and elementary functions (exp, log, sin, cos, etc.). By applying the chain rule repeatedly to these operations, partial derivatives of arbitrary order can be computed automatically, accurately to working precision, and using at most a small constant factor of more arithmetic operations than the original program.\"\n",
    "\n",
    "- Optimized for many inputs and few outputs (like neural networks)\n",
    "\n",
    "- This is what TensorFlow uses\n",
    "\n",
    "## SoftMax\n",
    "\n",
    "- Used for classification\n",
    "\n",
    "Converts weights of neural network into probabilities.\n",
    "\n",
    "\"Softmax function converts a vector of K real numbers into a probability distribution of K possible outcomes.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*n6sJ4yZQzwKL9wnF5wnVNg.png)\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*_Zy1C83cnmYUdETCeQrOgA.png)\n",
    "\n",
    "- Step function (activation function): return value > 0 then output = 1 (it is activated), else is 0 (not activated)\n",
    "\n",
    "- Weight  increase or decrease the input value. Weights can be positive or negative.\n",
    "\n",
    "- Perceptron is a layer of a linear classifier (binary). Perceptron is a single layer neural network and a multi-layer perceptron is called Neural Networks.\n",
    "\n",
    "    - Perceptron can learn by reinforcing weights that lead to correct behavior during training\n",
    "\n",
    "    - Deep Neural Network (deep learning): multi-layer perceptron\n",
    "\n",
    "\n",
    "**Modern Deep Neural Network**\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1400/1*ZXAOUqmlyECgfVa81Sr6Ew.png)\n",
    "\n",
    "- Replace step function by something better (relu for example)\n",
    "\n",
    "- Apply softmax function to he output (convert output to probabilities)\n",
    "\n",
    "- Training using gradient descent and more (for example moments)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "Is Gradient Descent using reverse mode auto-diff\n",
    "\n",
    "\"O que é: atualizar todos os pesos w das camadas a partir da última até atingir a camada de entrada da rede, sempre tendo em vista diminuir esse erro\"\n",
    "\n",
    "\"Strictly speaking, the term backpropagation refers only to an algorithm for efficiently computing the gradient\"\n",
    "\n",
    "Passos:\n",
    "\n",
    "- \"Inicializar todos os pesos da rede com pequenos valores aleatórios.\n",
    "\n",
    "- Fornecer dados de entrada à rede e calcular o valor da função de erro obtida, ao comparar com o valor de saída esperado. Lembre-se de que como o aprendizado é supervisionado, já se sabe de antemão qual deveria ser a resposta correta. É importante que a função de erro seja diferenciável.\n",
    "\n",
    "- Na tentativa de minimizar o valor da função de erro, calculam-se os valores dos gradientes para cada peso da rede. Do Cálculo, sabemos que o vetor gradiente fornece a direção de maior crescimento de uma função; aqui, como queremos caminhar com os pesos na direção de maior decréscimo da função de erro, basta tomarmos o sentido contrário ao do gradiente e…voilà! Já temos um excelente caminho por onde andar.\n",
    "\n",
    "- Uma vez que temos o vetor gradiente calculado, atualizamos cada peso de modo iterativo, sempre recalculando os gradientes em cada passo de iteração, até o erro diminuir e chegar abaixo de algum limiar preestabelecido, ou o número de iterações atingir um valor máximo, quando enfim o algoritmo termina e a rede está treinada.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation function (aka rectifier)\n",
    "\n",
    "- Step function don't work with gradient descent (there is no gradient)\n",
    "\n",
    "It termines de output giving some inputs (weights)\n",
    "\n",
    "Alternative functions:\n",
    "\n",
    "- Logistic (Sigmoid)\n",
    "\n",
    "- Hyperbolic tangent\n",
    "\n",
    "- Exponential linear unit - ELU\n",
    "\n",
    "- Rectified Linear Unit - RELU  *really common and fast \n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1400/1*XxxiA0jJvPrHEJHD4z893g.png)\n",
    "\n",
    "![](https://ars.els-cdn.com/content/image/1-s2.0-S016971611830021X-f09-17-9780444640420.jpg)\n",
    "\n",
    "![](https://www.researchgate.net/publication/350125835/figure/fig1/AS:1010752341168129@1617993545112/Sigmoid-Scaled-Exponential-Linear-Unit-SELU-Hyperbolic-tangent-TanH-Rectifier.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization functions\n",
    "\n",
    "Optimization functions:\n",
    "\n",
    "- Gradient Descent (GD)\n",
    "\n",
    "Faster than GD:\n",
    "\n",
    "- Momentum: Introduces momentum term to the descent so it slows downs as things start to flatten and speeds up as the slope is steep\n",
    "\n",
    "- Nesterov Accelerated Gradient: (a small tweak on momentum opt.) it computes momentum based on the gradient slightly ahead of you, not where you are, to take that into consideration\n",
    "\n",
    "- RMSProp: adaptative learning rate to help point toward minimum\n",
    "\n",
    "- Adam: adaptative moment estimation (momentum + RMSProp). *really popular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tunning topology\n",
    "\n",
    "- One way: Trial and error\n",
    "\n",
    "    - Evaluate a smaller network with less neurons in the hidden layers: Try increasing the size\n",
    "\n",
    "    - Evaluate a larger network with more layers: Try reducing the size of each layer as you progress form a funnel\n",
    "\n",
    "- More layers can yield faster learning\n",
    "\n",
    "- Use more layers and neurons than you need because you will use early stopping\n",
    "\n",
    "- 'Model zoos': libraries to specific problems with optimal topologies previously tested"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting\n",
    "\n",
    "- Too complex nn (too much weights), can happens to overfit\n",
    "\n",
    "Prevent it:\n",
    "\n",
    "- Early stopping (when performance starts dropping)\n",
    "\n",
    "- Regularization terms added to cost function during training: \"*Regularization is a technique that penalizes the coefficient. In an overfit model, the coefficients are generally inflated. Thus, Regularization adds penalties to the parameters and avoids them weigh heavily. The coefficients are added to the cost function of the linear equation. Thus, if the coefficient inflates, the cost function will increase.*\"\n",
    "\n",
    "- Dropout - ignore all neurons randomly at each training step \"*Dropout is applied to a neural network by randomly dropping neurons in every layer (including the input layer). A pre-defined dropout rate determines the chance of each neuron being dropped.*\"\n",
    "\n",
    "    - Works well and forces your model to spread out its learning"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
