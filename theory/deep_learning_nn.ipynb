{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "ML Optimization algorithm to achive the local minimun.\n",
    "\n",
    "\"Gradient descent is an optimization algorithm that's used when training a machine learning model. It's based on a convex function and tweaks its parameters iteratively to minimize a given function to its local minimum.\"\n",
    "\n",
    "How it works: choose random parameter values ​​and check the accuracy, then go to close parameter values ​​and check the accuracy and see if the loss function results have decreased. It keeps repeating iteratively that the unit reaches the minimum.\n",
    "\n",
    "Problem: how make sure that the local minimum is the global minimum?\n",
    "\n",
    "    - In practice is not a big problem\n",
    "\n",
    "## Autodiff (rervese mode autodiff)\n",
    "\n",
    "- Technique to speed Gradient Descent.\n",
    "\n",
    "\"Autodiff is a set of techniques to evaluate the partial derivative of a function specified by a computer program.\"\n",
    "\n",
    "\"Automatic differentiation exploits the fact that every computer calculation, no matter how complicated, executes a sequence of elementary arithmetic operations (addition, subtraction, multiplication, division, etc.) and elementary functions (exp, log, sin, cos, etc.). By applying the chain rule repeatedly to these operations, partial derivatives of arbitrary order can be computed automatically, accurately to working precision, and using at most a small constant factor of more arithmetic operations than the original program.\"\n",
    "\n",
    "- Optimized for many inputs and few outputs (like neural networks)\n",
    "\n",
    "- This is what TensorFlow uses\n",
    "\n",
    "## SoftMax\n",
    "\n",
    "- Used for classification\n",
    "\n",
    "Converts weights of neural network into probabilities.\n",
    "\n",
    "\"Softmax function converts a vector of K real numbers into a probability distribution of K possible outcomes.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*n6sJ4yZQzwKL9wnF5wnVNg.png)\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*_Zy1C83cnmYUdETCeQrOgA.png)\n",
    "\n",
    "- Step function (activation function): return value > 0 then output = 1 (it is activated), else is 0 (not activated)\n",
    "\n",
    "- Weight  increase or decrease the input value. Weights can be positive or negative.\n",
    "\n",
    "- Perceptron is a layer of a linear classifier (binary). Perceptron is a single layer neural network and a multi-layer perceptron is called Neural Networks.\n",
    "\n",
    "    - Perceptron can learn by reinforcing weights that lead to correct behavior during training\n",
    "\n",
    "    - Deep Neural Network (deep learning): multi-layer perceptron\n",
    "\n",
    "\n",
    "**Modern Deep Neural Network**\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1400/1*ZXAOUqmlyECgfVa81Sr6Ew.png)\n",
    "\n",
    "- Replace step function by something better (relu for example)\n",
    "\n",
    "- Apply softmax function to he output (convert output to probabilities)\n",
    "\n",
    "- Training using gradient descent and more (for example moments)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
