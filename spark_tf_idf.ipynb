{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Spark Term Frequency for Search Algorithms\n",
        "\n",
        "TF-IDF: Term Frequency and Invert Document Frequency\n",
        "\n",
        "- Figures out what terms are most relevant for a document\n",
        "\n",
        "**Term Frequency**: measures how often a word occurs in a document. (a word that occurs frequently is probably important to that document's meaning)\n",
        "\n",
        "**Document Frenquecy**: is how often a word occurs in an entire set of documents, i.e., all of Wikipedia or every web page. (Common words that just appear everywhere no matter what the toping, like 'a', 'the', 'and'...)\n",
        "\n",
        "Relevancy of a word to a document: $\\frac{\\text{Term Frequency}}{\\text{Document Frequency}}$\n",
        "\n",
        "or $\\text{Term Frequency}*\\text{Inverse Document Frequency}$\n",
        "\n",
        "This is how often the word appears in a document, over, how often it just appears everywhere. That gives you a measure of how important and unique this word is for this document.\n",
        "\n",
        "## In Practice\n",
        "\n",
        "We use the log of the IDF, since word frequencies are distributed exponentially. That gives us a better weighting of a words overall popularity.\n",
        "\n",
        "TF-IDF assumes a document is just a 'bunch of words':\n",
        "- Parsing documents into a 'bunch of words' can be most of the owrk\n",
        "- Words can be represented as hash value (number) for efficiency\n",
        "- What about synonyms? Various tenses? Abbreviantions? Capitalizations? Misspellings?\n",
        "\n",
        "An simple search algorithm could be:\n",
        "\n",
        "1) ComputeTD-IDF for every word in a corpus\n",
        "\n",
        "2) For a given search word, sort the documents by their TF-IDF score for that word\n",
        "\n",
        "3)  Display the results\n"
      ],
      "metadata": {
        "id": "2CXlYF27fM5X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Search Wikipedia with Spark"
      ],
      "metadata": {
        "id": "M_nQanAWj12c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FccMKN8zfJ-e",
        "outputId": "349b8534-77d4-4049-a86a-41dc244fe4e1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=20bd6245fb4be0db9fd7fce68c6a4dabde0a11a8cecc6d5d1f524c9da9e981d3\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "eeD_0sv7e6Ep"
      },
      "outputs": [],
      "source": [
        "from pyspark.mllib.feature import HashingTF, IDF\n",
        "from pyspark import SparkConf, SparkContext"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conf = SparkConf().setMaster(\"local\").setAppName(\"SparkTFIDF\")\n",
        "sc = SparkContext(conf = conf)"
      ],
      "metadata": {
        "id": "NULs0Cewpm5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rawData = sc.textFile(\"subset-small.tsv\")\n",
        "fields = rawData.map(lambda x: x.split(\"\\t\"))\n",
        "documents = fields.map(lambda x: x[3].split(\" \"))"
      ],
      "metadata": {
        "id": "aI_-6_Wqkc60"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documentNames = fields.map(lambda x: x[1])"
      ],
      "metadata": {
        "id": "uy_L8gjgkvEV"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hashingTF = HashingTF(100000)\n",
        "tf = hashingTF.transform(documents)"
      ],
      "metadata": {
        "id": "h28RaTCilC9c"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TF*IDF\n",
        "tf.cache()\n",
        "idf = IDF(minDocFreq=2).fit(tf)\n",
        "tfidf = idf.transform(tf)"
      ],
      "metadata": {
        "id": "DiChDXgBlNbQ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gettysburgTF = hashingTF.transform([\"Gettysburg\"])\n",
        "gettysburgHashValue = int(gettysburgTF.indices[0])"
      ],
      "metadata": {
        "id": "Qjyu4loalaTz"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gettysburgRelevance = tfidf.map(lambda x: x[gettysburgHashValue])"
      ],
      "metadata": {
        "id": "cc1NUF0JmllR"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zippedResults = gettysburgRelevance.zip(documentNames)"
      ],
      "metadata": {
        "id": "Mwx3VsOgmuOt"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best document for Gettysburg is:\")\n",
        "print(zippedResults.max())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrC7KEZcmzGy",
        "outputId": "0d05c67b-94e9-47e9-8a95-86ec1c1e85ff"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best document for Gettysburg is:\n",
            "(0.0, 'Ælle of Sussex')\n"
          ]
        }
      ]
    }
  ]
}