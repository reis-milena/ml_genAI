{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Spark\n",
        "\n",
        "\"A fast and general engine for large-scale data processing.\"\n",
        "\n",
        "Distributes the work/operations on clusters (spread the work) - Resilient Distributed Dataset (RDD)\n",
        "\n",
        "![](https://www.researchgate.net/publication/327926641/figure/fig2/AS:675666567647232@1538102871025/Execution-model-of-Spark.png)\n",
        "\n",
        "Cluster manager: Spark"
      ],
      "metadata": {
        "id": "ionFxt_nee13"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RDD - Resilient Distributed Dataset\n",
        "\n",
        "\"A Resilient Distributed Dataset (RDD) is a read-only collection of data in Spark that can be partitioned across multiple machines in a cluster, allowing for parallel computation and fault tolerance through lineage reconstruction.\"\n",
        "\n",
        "Transforming RDD's\n",
        "\n",
        "- map\n",
        "\n",
        "- flatmap\n",
        "\n",
        "- filter\n",
        "\n",
        "- distinct\n",
        "\n",
        "- sample\n",
        "\n",
        "- union, intersection, substract, cartesian"
      ],
      "metadata": {
        "id": "-Xq64Rc6hOGI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtiYwLBAyv-T"
      },
      "outputs": [],
      "source": [
        "# map example\n",
        "rdd = sc.parallelize([1,2,3,4])\n",
        "rdd.map(lambda x: x*x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RDD Actions\n",
        "\n",
        "- collect\n",
        "\n",
        "- count\n",
        "\n",
        "- countByValue (unique values)\n",
        "\n",
        "- take\n",
        "\n",
        "- top\n",
        "\n",
        "- reduce (combining..)\n",
        "\n",
        "\n",
        "Nothing actually happens in your driver program util action is called!\n"
      ],
      "metadata": {
        "id": "h9JtLulTjyjj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLLibs - Machine Learning libraries\n",
        "\n",
        "MLLib Capabilites:\n",
        "\n",
        "- Feature extraction: term frequency, inverse document frequency\n",
        "- Basic statistics: chi-squared test, pearson or spearman correlation, min, max, mean, variance\n",
        "- Linear regression, logistic regression\n",
        "- Support Vector Machines (SVM)\n",
        "- Naive Bayes classifier\n",
        "- Decision trees\n",
        "- K-Means clustering\n",
        "- Principal component analysis, singular value decomposition (svd)\n",
        "- Recommendations using Alternating Least Squares\n",
        "\n",
        "MLLib Data types:\n",
        "\n",
        "- Vector (dense or sparse - only store data that exists)\n",
        "- LabeledPoint\n",
        "- Rating"
      ],
      "metadata": {
        "id": "SQitJ213kTLF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision tree with MLLib"
      ],
      "metadata": {
        "id": "204fYPs7mg-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qejrX3p1m-z7",
        "outputId": "5a7e0875-3618-42b1-c798-f8dc606e77e1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=7948a993b9abdb866d18e715dfb0e26b37df4b8c5d4e989bf5c5ffca14f4761b\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.mllib.regression import LabeledPoint\n",
        "from pyspark.mllib.tree import DecisionTree\n",
        "from pyspark import SparkConf, SparkContext\n",
        "from numpy import array"
      ],
      "metadata": {
        "id": "Squ-KEKGmko5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conf = SparkConf().setMaster('local').setAppName('SparkDecisionTree')\n",
        "sc = SparkContext(conf = conf)"
      ],
      "metadata": {
        "id": "60N7wq7DndHW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def binary(YN):\n",
        "  if (YN=='Y'):\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "def mapEducation(degree):\n",
        "  if (degree == 'BS'):\n",
        "    return 1\n",
        "  elif (degree == 'MS'):\n",
        "    return 2\n",
        "  elif (degree == \"PhD\"):\n",
        "    return 3\n",
        "  else:\n",
        "    return 0"
      ],
      "metadata": {
        "id": "2gL6TVbynsA2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def createLabeledPoints(fields):\n",
        "  yearsExperience = int(fields[0])\n",
        "  employed = binary(fields[1])\n",
        "  previousEmployers = int(fields[2])\n",
        "  educationLevel = mapEducation(fields[3])\n",
        "  topTier = binary(fields[4])\n",
        "  interned = binary(fields[5])\n",
        "  hired = binary(fields[6])\n",
        "\n",
        "  return LabeledPoint(hired, array([yearsExperience, employed, previousEmployers,\n",
        "                                    educationLevel, topTier, interned]))"
      ],
      "metadata": {
        "id": "Olm_84Mwoqoq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rawData = sc.textFile(\"PastHires.csv\") #rdd\n",
        "header = rawData.first()\n",
        "rawData = rawData.filter(lambda x: x!= header) #filter out the column names"
      ],
      "metadata": {
        "id": "o38-S3zxpcwy"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#split each line into a list based on the comma delimiters\n",
        "csvData = rawData.map(lambda x: x.split(\",\"))"
      ],
      "metadata": {
        "id": "OqL5bdmLp4er"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert these lists to labeled points\n",
        "trainingData = csvData.map(createLabeledPoints)"
      ],
      "metadata": {
        "id": "bAgInmOYqBjQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a test candidate with 10 years of experience, currently employed, with a BS but from a non-top-tier school and did not do an internship"
      ],
      "metadata": {
        "id": "Y1EajDG1rawy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testCandidates = [array([10,1,3,1,0,0])]\n",
        "testData = sc.parallelize(testCandidates)"
      ],
      "metadata": {
        "id": "eVk_zWTuraaf"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DecisionTree.trainClassifier(trainingData, numClasses=2,\n",
        "                                     categoricalFeaturesInfo={1:2,3:4,4:2,5:2},\n",
        "                                     impurity='gini',\n",
        "                                     maxDepth=5,\n",
        "                                     maxBins=32)"
      ],
      "metadata": {
        "id": "t4JaRp8Hr678"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(testData)\n",
        "print('Hire prediction: ')\n",
        "results = predictions.collect()\n",
        "for result in results:\n",
        "  print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tL0dEIE6sKev",
        "outputId": "fafed58e-b3ea-46e5-9e39-28a5f8e62607"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hire prediction: \n",
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Learned classification tree model:\")\n",
        "print(model.toDebugString())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6jXUjl2sZvI",
        "outputId": "d549ae9e-b0a5-43c3-d484-505e16890206"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learned classification tree model:\n",
            "DecisionTreeModel classifier of depth 4 with 9 nodes\n",
            "  If (feature 1 in {0.0})\n",
            "   If (feature 5 in {0.0})\n",
            "    If (feature 0 <= 0.5)\n",
            "     If (feature 3 in {1.0})\n",
            "      Predict: 0.0\n",
            "     Else (feature 3 not in {1.0})\n",
            "      Predict: 1.0\n",
            "    Else (feature 0 > 0.5)\n",
            "     Predict: 0.0\n",
            "   Else (feature 5 not in {0.0})\n",
            "    Predict: 1.0\n",
            "  Else (feature 1 not in {0.0})\n",
            "   Predict: 1.0\n",
            "\n"
          ]
        }
      ]
    }
  ]
}