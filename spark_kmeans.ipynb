{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# K-Means with Spark"
      ],
      "metadata": {
        "id": "CVANRR6SY8xL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXKQkRwVYyrq",
        "outputId": "8516b5ba-68ea-4a89-e725-baa2f51273c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=4f67f8a9dca2ab44d0c7fb588c93a6622a8919c33dd1dc265f9d692bcc028959\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.mllib.clustering import KMeans\n",
        "from pyspark import SparkConf, SparkContext\n",
        "from numpy import array, random\n",
        "from math import sqrt\n",
        "from sklearn.preprocessing import scale"
      ],
      "metadata": {
        "id": "dUHV4qIvZOWo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conf = SparkConf().setMaster('local').setAppName('SparkKMeans')\n",
        "sc = SparkContext(conf = conf)"
      ],
      "metadata": {
        "id": "XDJAm1i5ZylA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def createClusteredData(N,k):\n",
        "    random.seed(10)\n",
        "    pointsPerCluster = float(N)/k\n",
        "    X = []\n",
        "    for i in range(k):\n",
        "        incomeCentroid = random.uniform(20000.0, 200000.0)\n",
        "        ageCentroid = random.uniform(20.0, 70.0)\n",
        "        for j in range(int(pointsPerCluster)):\n",
        "            X.append([random.normal(incomeCentroid,10000.0),\n",
        "                      random.normal(ageCentroid, 2.0)])\n",
        "    X = array(X)\n",
        "\n",
        "    return X"
      ],
      "metadata": {
        "id": "Tildm6C_Zrz-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "K=5\n",
        "data = sc.parallelize(scale(createClusteredData(100,K)))"
      ],
      "metadata": {
        "id": "Sdd44Q5aZ40g"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clusters = KMeans.train(data, K,\n",
        "                        maxIterations=10,\n",
        "                        #runs = 10,\n",
        "                        initializationMode='random')"
      ],
      "metadata": {
        "id": "PXpgtEGraJd3"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resultRDD = data.map(lambda point: clusters.predict(point)).cache()\n",
        "print(\"Counts by value: \") #count unique cluster\n",
        "counts = resultRDD.countByValue()\n",
        "print(counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubqNWm2gauqM",
        "outputId": "10973993-10fe-431a-d1ab-cad8195c9aba"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counts by value: \n",
            "defaultdict(<class 'int'>, {0: 20, 2: 14, 1: 6, 3: 40, 4: 20})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Cluster assignments\")\n",
        "results = resultRDD.collect()\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCZKqyUxbNgh",
        "outputId": "10435a01-dde7-496e-d7d7-5b5d908351e8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster assignments\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 1, 2, 2, 1, 1, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate clustering by computing Within Set Sum of Squared Errors\n",
        "def error(point):\n",
        "  center = clusters.centers[clusters.predict(point)]\n",
        "  return sqrt(sum([x**2 for x in (point - center)]))"
      ],
      "metadata": {
        "id": "EJkggqmubW4P"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WSSSE = data.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
        "print(\"Within Set Sum of Squared Error = \"+ str(WSSSE))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNdn9zc0cAAU",
        "outputId": "1be161eb-bda0-46c6-a496-725c75a6909d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Within Set Sum of Squared Error = 22.25445343635214\n"
          ]
        }
      ]
    }
  ]
}