{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "- An agent that 'explores' some space\n",
    "\n",
    "- As it goes, it learns the value of different state changes in different conditions\n",
    "\n",
    "- Those values inform subsequente behavior of the agent\n",
    "\n",
    "## Q-Learning\n",
    "\n",
    "\"Q-learning is a model-free reinforcement learning algorithm to learn the value of an action in a particular state. It does not require a model of the environment (hence \"model-free\")\"\n",
    "\n",
    "An implementation of reinforement learning\n",
    "\n",
    "- Set of environmental states $s$\n",
    "\n",
    "- Set of actions per state $A$. By performing an action ${\\displaystyle a\\in {A}}$, the agent transitions from state to state.\n",
    "\n",
    "- Value of each state/action $Q$. (Executing an action in a specific state provides the agent with a reward -a numerical score)\n",
    "\n",
    "1) Start off with $Q$ values of 0\n",
    "\n",
    "2) Explore the space\n",
    "\n",
    "3) When **bad** things happens after a given state/action, it **reduces** its $Q$\n",
    "\n",
    "4) When **good** things happens after a given state/action, it **increases** its $Q$\n",
    "\n",
    "\"**The goal of the agent is to maximize its total reward**. It does this by adding the maximum reward attainable from future states to the reward for achieving its current state, effectively influencing the current action by the potential future reward. This potential reward is a weighted sum of expected values of the rewards of all future steps starting from the current state\"\n",
    "\n",
    "![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSNjoZXhylJVR1BKfio21ntpgj9pUmPcKOFlw&s)\n",
    "\n",
    "<!---  $Q_t(s,a) = Q_{t-1}(s,a)+\\alpha(R(s,a)+\\gamma R(s'))$\n",
    "\n",
    "$Q_t(s,a) = Q_{t-1}(s,a)+\\alpha TD_t(a,s)$ --->\n",
    "\n",
    "$Q(s,a) += \\alpha * (R(s,a) + \\max(Q(s')) - Q(s,a))$ \n",
    "\n",
    "where $s$ is previous state, $s'$ is the current state and $\\alpha$ is the discount factor.\n",
    "\n",
    "new_q = (1-learning_rate) * prev_q + learning_rate * (reward + discount_factor*next_max_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The exploration problem\n",
    "\n",
    "How do we efficiently explore all of the possible states?\n",
    "\n",
    "- Simple approach: always choose the action for a given state with the highest Q. If there's a tie, choose at random. (It's inefficient as you might miss a lot of paths)\n",
    "\n",
    "- Introduce an epsilon term:\n",
    "\n",
    "    - If a random number is less than $\\epsilon$, don't follow the highest Q but choose at random\n",
    "\n",
    "    - That way, exploration never totally stops\n",
    "\n",
    "    - But it can be tricky to choose $\\epsilon$\n",
    "\n",
    "**This is Markov Decision Process!**\n",
    "\n",
    "\"A Markov chain is a sequence of states where the probability of moving to the next state depends only on the current state and not on the sequence of events that preceded it.\"\n",
    "\n",
    "\"In an MDP, an agent makes decisions that influence the transitions between states. Each decision (or action) taken in a particular state leads to a probability distribution over the next possible states, similar to a Markov chain. However, unlike a simple Markov chain, in an MDP, the agent can actively choose actions to optimize a certain objective (usually maximizing some cumulative reward).\"\n",
    "\n",
    "**This is also Dynamic Programming!**\n",
    "\n",
    "\"It refers to simplifying a complicated problem by breaking it down into simpler sub-problems in a recursive manner.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning with gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[43mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random.seed(1234)\n",
    "\n",
    "streets = gym.make('Taxi-v3').env\n",
    "streets.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actions: 0 = south, 1 = north, 2 = east, 3 = west, 4 = pickup, 5 = dropoff\n",
    "\n",
    "<img src=\"https://media.istockphoto.com/id/1151362772/pt/vetorial/diagram-compass-rose-for-navigation-orientation.jpg?s=612x612&w=0&k=20&c=xICp8-cqYnvPir4UB54lvkqIfrDcScQKtjeo79dGnjE=\" alt=\"Drawing\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining initial state at (2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (East)\n"
     ]
    }
   ],
   "source": [
    "#(2 - axis X,\n",
    "# 3 - axis Y,\n",
    "# 2 - where pick up passenger,\n",
    "# 0 - destination)\n",
    "initial_state = streets.encode(2,3,2,0)\n",
    "streets.s = initial_state\n",
    "streets.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 368, -1, False)],\n",
       " 1: [(1.0, 168, -1, False)],\n",
       " 2: [(1.0, 288, -1, False)],\n",
       " 3: [(1.0, 248, -1, False)],\n",
       " 4: [(1.0, 268, -10, False)],\n",
       " 5: [(1.0, 268, -10, False)]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reward table for each action (0 = south, 1 = north, 2 = east, 3 = west, 4 = pickup, 5 = dropoff)\n",
    "streets.P[initial_state]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "action: [(**prob. assign** to that action, **next state result** for that action, **reward** for that action, sucessful **to dropoff**)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.zeros([streets.observation_space.n, streets.action_space.n])\n",
    "\n",
    "#hyperparameters\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.6\n",
    "exploration = 0.1\n",
    "epochs = 10000\n",
    "\n",
    "for taxi_run in range(epochs):\n",
    "    state = streets.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        random_value = random.uniform(0,1)\n",
    "        if (random_value < exploration):\n",
    "            action = streets.action_space.sample() #explore a random action\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])\n",
    "        \n",
    "        next_state, reward, done, info = streets.step(action)\n",
    "\n",
    "        prev_q = q_table[state, action]\n",
    "        next_max_q = np.max(q_table[next_state])\n",
    "        new_q = (1-learning_rate) * prev_q + learning_rate * (reward + discount_factor*next_max_q)\n",
    "        q_table[state,action] = new_q\n",
    "\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.40128866, -2.39738234, -2.40248868, -2.3639511 , -6.33514305,\n",
       "       -6.86827889])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table[initial_state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trip number 10\n",
      "+---------+\n",
      "|R: | : :\u001b[35m\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "for tripnum in range (1,11):\n",
    "    state = streets.reset()\n",
    "\n",
    "    done= False\n",
    "\n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])\n",
    "        next_state, reward, done, info = streets.step(action)\n",
    "        clear_output(wait=True)\n",
    "        print(\"Trip number \"+str(tripnum))\n",
    "        print(streets.render(mode='ansi'))\n",
    "        sleep(.5)\n",
    "        state=next_state\n",
    "    sleep(2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
